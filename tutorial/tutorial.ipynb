{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74d66b9",
   "metadata": {},
   "source": [
    "### Model Adaptation & Behavior Controlling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35446362",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and execute if not done in README.ipynb\n",
    "# !pip install -r default_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17479e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory if not in tutorial\n",
    "# import os\n",
    "# os.chdir(\"./tutorial\")\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440fbfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['HF_TOKEN'] = \"\"\n",
    "# os.environ['HF_HOME']=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f7fdc",
   "metadata": {},
   "source": [
    "### Quality of Model Response\n",
    "\n",
    "You yourself can run the following cells and see how quality of response being affected by SFT and GRPO.\n",
    "\n",
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfee6f-a677-4fe0-bf91-23637f5f8433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5b15d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-19 11:19:26 [__init__.py:248] Automatically detected platform rocm.\n",
      "PyTorch detected number of available devices: 1\n",
      "============================================================\n",
      "BLOOD RELATIONS GRPO TRAINER\n",
      "============================================================\n",
      "Training Type: grpo\n",
      "Mode: inference\n",
      "Model: /jupyter-tutorial/hf_models/Llama-3.2-1B-Instruct\n",
      "Output directory: checkpoints/demo/sft\n",
      "Dataset file: formatted_questions_array.json\n",
      "Learning rate: 2e-05\n",
      "Epochs: 3\n",
      "Batch size: 1\n",
      "LoRA rank: 8\n",
      "LoRA alpha: 16\n",
      "Max sequence length: 512\n",
      "GPU IDs: 0\n",
      "============================================================\n",
      "============================================================\n",
      "RUNNING BATCH INFERENCE\n",
      "============================================================\n",
      "Loading questions from: test_questions_array.json\n",
      "Successfully loaded 10 questions from JSON file\n",
      "Processing 10 questions...\n",
      "Setting up inference model...\n",
      "Setting up inference model on device: cuda\n",
      "Loading base model: /jupyter-tutorial/hf_models/Llama-3.2-1B-Instruct\n",
      "Loading tokenizer for: /jupyter-tutorial/hf_models/Llama-3.2-1B-Instruct\n",
      "Found 1 checkpoints. Using latest: checkpoints/demo/sft/checkpoint-603\n",
      "Loading LoRA adapters from: checkpoints/demo/sft/checkpoint-603\n",
      "Inference model setup completed successfully\n",
      "Found 1 checkpoints. Using latest: checkpoints/demo/sft/checkpoint-603\n",
      "Processing question 1/10...\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Response generated in 22.41s. Length: 136 chars.\n",
      "Processing question 2/10...\n",
      "Response generated in 1.88s. Length: 108 chars.\n",
      "Processing question 3/10...\n",
      "Response generated in 1.95s. Length: 100 chars.\n",
      "Processing question 4/10...\n",
      "Response generated in 3.30s. Length: 246 chars.\n",
      "Processing question 5/10...\n",
      "Response generated in 1.76s. Length: 117 chars.\n",
      "Processing question 6/10...\n",
      "Response generated in 3.01s. Length: 192 chars.\n",
      "Processing question 7/10...\n",
      "Response generated in 2.26s. Length: 119 chars.\n",
      "Processing question 8/10...\n",
      "Response generated in 2.15s. Length: 161 chars.\n",
      "Processing question 9/10...\n",
      "Response generated in 2.81s. Length: 174 chars.\n",
      "Processing question 10/10...\n",
      "Response generated in 12.81s. Length: 718 chars.\n",
      "Batch inference complete. Results saved to simple_inference.md\n",
      "Answer Accuracy: 4/10 (40.00%)\n",
      "Format Accuracy: 10/10 (100.00%)\n",
      "Inference model cleaned up\n",
      "[rank0]:[W719 11:20:23.976995514 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Base model:\n",
    "!python -m trainer --mode inference --inference_output simple_inference.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e2c9721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-19 18:30:56 [__init__.py:248] Automatically detected platform rocm.\n",
      "PyTorch detected number of available devices: 1\n",
      "============================================================\n",
      "BLOOD RELATIONS GRPO TRAINER\n",
      "============================================================\n",
      "Training Type: grpo\n",
      "Mode: inference\n",
      "Model: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Output directory: checkpoints/sft\n",
      "Dataset file: formatted_questions_array.json\n",
      "Learning rate: 2e-05\n",
      "Epochs: 3\n",
      "Batch size: 1\n",
      "LoRA rank: 8\n",
      "LoRA alpha: 16\n",
      "Max sequence length: 512\n",
      "GPU IDs: 0\n",
      "============================================================\n",
      "============================================================\n",
      "RUNNING BATCH INFERENCE\n",
      "============================================================\n",
      "Loading questions from: test_questions_array.json\n",
      "Successfully loaded 10 questions from JSON file\n",
      "Processing 10 questions...\n",
      "Setting up inference model...\n",
      "Setting up inference model on device: cuda\n",
      "Loading base model: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.10s/it]\n",
      "Loading tokenizer for: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Found 1 checkpoints. Using latest: checkpoints/sft/checkpoint-9\n",
      "Loading LoRA adapters from: checkpoints/sft/checkpoint-9\n",
      "Inference model setup completed successfully\n",
      "Found 1 checkpoints. Using latest: checkpoints/sft/checkpoint-9\n",
      "Processing question 1/10...\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sft-trained model:\n",
    "!python -m trainer --mode inference --output_dir checkpoints/sft --inference_output sft_inference.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fad6a949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-19 18:25:00 [__init__.py:248] Automatically detected platform rocm.\n",
      "PyTorch detected number of available devices: 1\n",
      "============================================================\n",
      "BLOOD RELATIONS GRPO TRAINER\n",
      "============================================================\n",
      "Training Type: grpo\n",
      "Mode: inference\n",
      "Model: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Output directory: checkpoints/grpo\n",
      "Dataset file: formatted_questions_array.json\n",
      "Learning rate: 2e-05\n",
      "Epochs: 3\n",
      "Batch size: 1\n",
      "LoRA rank: 8\n",
      "LoRA alpha: 16\n",
      "Max sequence length: 512\n",
      "GPU IDs: 0\n",
      "============================================================\n",
      "============================================================\n",
      "RUNNING BATCH INFERENCE\n",
      "============================================================\n",
      "Loading questions from: test_questions_array.json\n",
      "Successfully loaded 10 questions from JSON file\n",
      "Processing 10 questions...\n",
      "Setting up inference model...\n",
      "Setting up inference model on device: cuda\n",
      "Loading base model: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.09s/it]\n",
      "Loading tokenizer for: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Found 1 checkpoints. Using latest: checkpoints/grpo/checkpoint-20\n",
      "Loading LoRA adapters from: checkpoints/grpo/checkpoint-20\n",
      "Inference model setup completed successfully\n",
      "Found 1 checkpoints. Using latest: checkpoints/grpo/checkpoint-20\n",
      "Processing question 1/10...\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Response generated in 124.62s. Length: 2835 chars.\n",
      "Processing question 2/10...\n",
      "Response generated in 60.28s. Length: 1470 chars.\n",
      "Processing question 3/10...\n",
      "Response generated in 53.54s. Length: 1317 chars.\n",
      "Processing question 4/10...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Grpo-trained model:\n",
    "!python -m trainer --mode inference --output_dir checkpoints/grpo --inference_output grpo_inference.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b57f9",
   "metadata": {},
   "source": [
    "\n",
    "#### SFT (Supervised Fine-tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c68681cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-19 21:01:00 [__init__.py:248] Automatically detected platform rocm.\n",
      "PyTorch detected number of available devices: 1\n",
      "============================================================\n",
      "BLOOD RELATIONS SFT TRAINER\n",
      "============================================================\n",
      "Training Type: sft\n",
      "Mode: train\n",
      "Model: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Output directory: checkpoints/seating/sft\n",
      "Dataset file: ../outputs/seating_question.json\n",
      "Learning rate: 2e-05\n",
      "Epochs: 10\n",
      "Batch size: 4\n",
      "LoRA rank: 8\n",
      "LoRA alpha: 32.0\n",
      "Max sequence length: 512\n",
      "GPU IDs: 0\n",
      "============================================================\n",
      "Starting SFT training...\n",
      "Loading dataset from: ../outputs/seating_question.json\n",
      "âœ“ Loaded question 1: Who lives second left of person living in Ranchi?...\n",
      "âœ“ Loaded question 2: Which of the following statement is false about Ri...\n",
      "âœ“ Loaded question 3: Who live in Kolkata?...\n",
      "Successfully loaded 25 questions from JSON file\n",
      "Created 25 SFT training samples\n",
      "Loading model: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 75.97it/s]\n",
      "Model and tokenizer loaded successfully\n",
      "Starting SFT training...\n",
      "Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 9416.09 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1229.99 examples/s]\n",
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 15463.44 examples/s]\n",
      "[rank0]:[W719 21:01:05.847408510 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s][rank0]:[W719 21:01:24.222503859 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "{'loss': 2.4076, 'grad_norm': 3.6844935417175293, 'learning_rate': 1.9606851875768404e-05, 'num_tokens': 13939.0, 'mean_token_accuracy': 0.628272557258606, 'epoch': 1.43}\n",
      "{'loss': 1.9175, 'grad_norm': 1.978956937789917, 'learning_rate': 1.731531335263669e-05, 'num_tokens': 27515.0, 'mean_token_accuracy': 0.6465481400489808, 'epoch': 2.86}\n",
      "{'loss': 1.5434, 'grad_norm': 1.4909862279891968, 'learning_rate': 1.344466850284333e-05, 'num_tokens': 40324.0, 'mean_token_accuracy': 0.6790941655635834, 'epoch': 4.29}\n",
      "{'loss': 1.2978, 'grad_norm': 1.6391698122024536, 'learning_rate': 8.830446780279175e-06, 'num_tokens': 54142.0, 'mean_token_accuracy': 0.7194171905517578, 'epoch': 5.71}\n",
      "{'loss': 1.1161, 'grad_norm': 1.4510128498077393, 'learning_rate': 4.468688458748006e-06, 'num_tokens': 66898.0, 'mean_token_accuracy': 0.7508253693580628, 'epoch': 7.14}\n",
      "{'loss': 1.0323, 'grad_norm': 1.432328224182129, 'learning_rate': 1.300936275912098e-06, 'num_tokens': 80503.0, 'mean_token_accuracy': 0.7647844016551971, 'epoch': 8.57}\n",
      "{'loss': 0.9967, 'grad_norm': 1.8186559677124023, 'learning_rate': 1.099108514288627e-08, 'num_tokens': 93220.0, 'mean_token_accuracy': 0.7752226114273071, 'epoch': 10.0}\n",
      "{'train_runtime': 42.7611, 'train_samples_per_second': 5.846, 'train_steps_per_second': 1.637, 'train_loss': 1.473062310900007, 'epoch': 10.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:42<00:00,  1.64it/s]\n",
      "Saving LoRA adapters to checkpoints/seating/sft\n",
      "SFT training completed successfully\n",
      "SFT training completed!\n",
      "Inference model cleaned up\n",
      "[rank0]:[W719 21:01:49.537504870 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# /jupyter-tutorial/hf_models/Llama-3.2-1B-Instruct\n",
    "!python -m trainer \\\n",
    "    --training_type sft \\\n",
    "    --mode train \\\n",
    "    --dataset_file \"../outputs/seating_question.json\"\\\n",
    "    --output_dir \"checkpoints/seating/sft\" \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --lora_r 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --disable_wandb # remove this or comment out for wandb logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b5fa8",
   "metadata": {},
   "source": [
    "#### RL (GRPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b6e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-19 18:22:37 [__init__.py:248] Automatically detected platform rocm.\n",
      "PyTorch detected number of available devices: 1\n",
      "============================================================\n",
      "BLOOD RELATIONS GRPO TRAINER\n",
      "============================================================\n",
      "Training Type: grpo\n",
      "Mode: train\n",
      "Model: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Output directory: checkpoints/grpo\n",
      "Dataset file: formatted_questions_array.json\n",
      "Learning rate: 1e-05\n",
      "Epochs: 2\n",
      "Batch size: 2\n",
      "LoRA rank: 16\n",
      "LoRA alpha: 32.0\n",
      "Max sequence length: 512\n",
      "GPU IDs: 0\n",
      "============================================================\n",
      "Starting GRPO training...\n",
      "Loading dataset from: formatted_questions_array.json\n",
      "âœ“ Loaded question 1: A is B's mother. B is C's only son. C is D's husba...\n",
      "âœ“ Loaded question 2: P's father, Q, is the only son of R's husband. R h...\n",
      "âœ“ Loaded question 3: K is married to L. L's brother, M, is the only son...\n",
      "Successfully loaded 10 questions from JSON file\n",
      "Created 10 GRPO training samples\n",
      "Loading model: /jupyter-tutorial/hf_models/Qwen3-4B\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 66.20it/s]\n",
      "Model and tokenizer loaded successfully\n",
      "Starting GRPO training...\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "INFO 07-19 18:22:42 [__init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 07-19 18:22:42 [__init__.py:32] name=lora_filesystem_resolver, value=vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 07-19 18:22:42 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 07-19 18:22:42 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "INFO 07-19 18:22:42 [__init__.py:44] plugin lora_filesystem_resolver loaded.\n",
      "INFO 07-19 18:22:51 [config.py:788] This model supports multiple tasks: {'classify', 'reward', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 07-19 18:22:51 [arg_utils.py:1601] rocm is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "INFO 07-19 18:22:51 [config.py:1829] Disabling V1 multiprocessing for external launcher.\n",
      "WARNING 07-19 18:22:51 [config.py:2161] max_num_batched_tokens (4096) exceeds max_num_seqs* max_model_len (2048). This may lead to unexpected behavior.\n",
      "INFO 07-19 18:22:51 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.6.dev315+g91a560098) with config: model='/jupyter-tutorial/hf_models/Qwen3-4B', speculative_config=None, tokenizer='/jupyter-tutorial/hf_models/Qwen3-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/jupyter-tutorial/hf_models/Qwen3-4B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, use_cached_outputs=False, \n",
      "INFO 07-19 18:22:52 [rocm.py:192] None is not supported in AMD GPUs.\n",
      "INFO 07-19 18:22:52 [rocm.py:193] Using ROCmFlashAttention backend.\n",
      "INFO 07-19 18:22:52 [parallel_state.py:1079] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-19 18:22:52 [model_runner.py:1173] Starting to load model /jupyter-tutorial/hf_models/Qwen3-4B...\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.44it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.02s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.05it/s]\n",
      "\n",
      "INFO 07-19 18:22:55 [default_loader.py:279] Loading weights took 2.96 seconds\n",
      "INFO 07-19 18:22:55 [model_runner.py:1205] Model loading took 7.5938 GiB and 3.065012 seconds\n",
      "[aiter] type hints mismatch, override to --> rmsnorm2d_fwd(arg0: torch.Tensor, arg1: torch.Tensor, arg2: float) -> torch.Tensor\n",
      "[aiter] type hints mismatch, override to --> rmsnorm2d_fwd_with_add(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: float) -> None\n",
      "INFO 07-19 18:23:08 [worker.py:320] Memory profiling takes 12.43 seconds\n",
      "INFO 07-19 18:23:08 [worker.py:320] the current vLLM instance can use total_gpu_memory (191.69GiB) x gpu_memory_utilization (0.70) = 134.18GiB\n",
      "INFO 07-19 18:23:08 [worker.py:320] model weights take 7.59GiB; non_torch_memory takes 0.34GiB; PyTorch activation peak memory takes 0.52GiB; the rest of the memory reserved for KV Cache is 125.73GiB.\n",
      "INFO 07-19 18:23:08 [executor_base.py:112] # rocm blocks: 57219, # CPU blocks: 1820\n",
      "INFO 07-19 18:23:08 [executor_base.py:117] Maximum concurrency for 512 tokens per request: 1788.09x\n",
      "INFO 07-19 18:23:09 [model_runner.py:1515] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.36it/s]\n",
      "INFO 07-19 18:23:10 [model_runner.py:1673] Graph capturing finished in 1 secs, took 0.14 GiB\n",
      "INFO 07-19 18:23:10 [llm_engine.py:438] init engine (profile, create kv cache, warmup model) took 14.44 seconds\n",
      "[rank0]:[W719 18:23:10.521381880 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.197, Total: -0.197\n",
      "[rank0]:[W719 18:23:20.393285509 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "{'loss': -0.0, 'grad_norm': 0.5663663148880005, 'learning_rate': 0.0, 'num_tokens': 2048.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.18575000762939453, 'rewards/_combined_reward_func/std': 0.015564383938908577, 'reward': -0.18575000762939453, 'reward_std': 0.015564383938908577, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.1}\n",
      "  5%|â–ˆâ–ˆâ–                                         | 1/20 [00:18<05:49, 18.39s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.345, Total: -0.345\n",
      "{'loss': 0.0, 'grad_norm': 0.5841115713119507, 'learning_rate': 1.0000000000000001e-07, 'num_tokens': 4096.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.33274999260902405, 'rewards/_combined_reward_func/std': 0.015840357169508934, 'reward': -0.33274999260902405, 'reward_std': 0.01584036462008953, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.2}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 2/20 [00:21<02:49,  9.44s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.177, Total: -0.177\n",
      "{'loss': -0.0, 'grad_norm': 0.31192195415496826, 'learning_rate': 2.0000000000000002e-07, 'num_tokens': 6144.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.1627500057220459, 'rewards/_combined_reward_func/std': 0.013073510490357876, 'reward': -0.1627500057220459, 'reward_std': 0.0130735132843256, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.3}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 3/20 [00:24<01:51,  6.58s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.130, Total: -0.130\n",
      "{'loss': 0.0, 'grad_norm': 0.4515831172466278, 'learning_rate': 3.0000000000000004e-07, 'num_tokens': 8192.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.12849999964237213, 'rewards/_combined_reward_func/std': 0.010847428813576698, 'reward': -0.12849999964237213, 'reward_std': 0.010847427882254124, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.4}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 4/20 [00:27<01:23,  5.25s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.233, Total: -0.233\n",
      "{'loss': 0.0, 'grad_norm': 0.4698869287967682, 'learning_rate': 4.0000000000000003e-07, 'num_tokens': 10240.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.24974998831748962, 'rewards/_combined_reward_func/std': 0.022969180718064308, 'reward': -0.24974998831748962, 'reward_std': 0.022969180718064308, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.5}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 5/20 [00:31<01:07,  4.50s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.180, Total: -0.180\n",
      "{'loss': 0.0, 'grad_norm': 0.45042985677719116, 'learning_rate': 5.000000000000001e-07, 'num_tokens': 12288.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.17350000143051147, 'rewards/_combined_reward_func/std': 0.013428826816380024, 'reward': -0.17350000143051147, 'reward_std': 0.013428819365799427, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.6}\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 6/20 [00:34<00:56,  4.06s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.125, Total: -0.125\n",
      "{'loss': -0.0, 'grad_norm': 0.4482768774032593, 'learning_rate': 6.000000000000001e-07, 'num_tokens': 14336.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.13625000417232513, 'rewards/_combined_reward_func/std': 0.02317146398127079, 'reward': -0.13625000417232513, 'reward_std': 0.02317146398127079, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.7}\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 7/20 [00:37<00:49,  3.77s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.174, Total: -0.174\n",
      "{'loss': -0.0, 'grad_norm': 0.47305014729499817, 'learning_rate': 7.000000000000001e-07, 'num_tokens': 16384.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.19574999809265137, 'rewards/_combined_reward_func/std': 0.04003643989562988, 'reward': -0.19574999809265137, 'reward_std': 0.04003643989562988, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.8}\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/20 [00:40<00:43,  3.58s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.228, Total: -0.228\n",
      "{'loss': -0.0, 'grad_norm': 0.5259483456611633, 'learning_rate': 8.000000000000001e-07, 'num_tokens': 18432.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.22175000607967377, 'rewards/_combined_reward_func/std': 0.03495115414261818, 'reward': -0.22175000607967377, 'reward_std': 0.03495115414261818, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.9}\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 9/20 [00:43<00:38,  3.46s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.175, Total: -0.175\n",
      "{'loss': 0.0, 'grad_norm': 0.40354546904563904, 'learning_rate': 9.000000000000001e-07, 'num_tokens': 20480.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.19349999725818634, 'rewards/_combined_reward_func/std': 0.07704761624336243, 'reward': -0.19349999725818634, 'reward_std': 0.07704760879278183, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.0}\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 10/20 [00:47<00:33,  3.37s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.165, Total: -0.165\n",
      "{'loss': 0.0, 'grad_norm': 0.49908122420310974, 'learning_rate': 1.0000000000000002e-06, 'num_tokens': 22528.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.19749999046325684, 'rewards/_combined_reward_func/std': 0.03183813393115997, 'reward': -0.19749999046325684, 'reward_std': 0.03183813393115997, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.1}\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 11/20 [00:50<00:29,  3.31s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.176, Total: -0.176\n",
      "{'loss': -0.0, 'grad_norm': 0.45626577734947205, 'learning_rate': 1.1e-06, 'num_tokens': 24576.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.1837500035762787, 'rewards/_combined_reward_func/std': 0.00801561214029789, 'reward': -0.1837500035762787, 'reward_std': 0.008015604689717293, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.2}\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 12/20 [00:53<00:26,  3.27s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.137, Total: -0.137\n",
      "{'loss': 0.0, 'grad_norm': 0.4549984931945801, 'learning_rate': 1.2000000000000002e-06, 'num_tokens': 26624.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.14774999022483826, 'rewards/_combined_reward_func/std': 0.012737742625176907, 'reward': -0.14774999022483826, 'reward_std': 0.012737742625176907, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.3}\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 13/20 [00:56<00:22,  3.25s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.187, Total: -0.187\n",
      "{'loss': 0.0, 'grad_norm': 0.5151174664497375, 'learning_rate': 1.3e-06, 'num_tokens': 28672.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.210999995470047, 'rewards/_combined_reward_func/std': 0.01971462555229664, 'reward': -0.210999995470047, 'reward_std': 0.01971462555229664, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.4}\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 14/20 [00:59<00:19,  3.23s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.281, Total: -0.281\n",
      "{'loss': 0.0, 'grad_norm': 0.49868616461753845, 'learning_rate': 1.4000000000000001e-06, 'num_tokens': 30720.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.26874998211860657, 'rewards/_combined_reward_func/std': 0.0178955290466547, 'reward': -0.26874998211860657, 'reward_std': 0.0178955290466547, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.5}\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 15/20 [01:02<00:16,  3.22s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.289, Total: -0.289\n",
      "{'loss': -0.0, 'grad_norm': 0.4339093863964081, 'learning_rate': 1.5e-06, 'num_tokens': 32768.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.3085000216960907, 'rewards/_combined_reward_func/std': 0.016360526904463768, 'reward': -0.3085000216960907, 'reward_std': 0.016360526904463768, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.6}\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 16/20 [01:06<00:12,  3.21s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.185, Total: -0.185\n",
      "{'loss': -0.0, 'grad_norm': 0.43984636664390564, 'learning_rate': 1.6000000000000001e-06, 'num_tokens': 34816.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.18025000393390656, 'rewards/_combined_reward_func/std': 0.020254626870155334, 'reward': -0.18025000393390656, 'reward_std': 0.020254628732800484, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.7}\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 17/20 [01:09<00:09,  3.20s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.131, Total: -0.131\n",
      "{'loss': -0.0, 'grad_norm': 0.4710245132446289, 'learning_rate': 1.7000000000000002e-06, 'num_tokens': 36864.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.1342500001192093, 'rewards/_combined_reward_func/std': 0.01804392598569393, 'reward': -0.1342500001192093, 'reward_std': 0.018043922260403633, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.8}\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/20 [01:12<00:06,  3.20s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.193, Total: -0.193\n",
      "{'loss': 0.0, 'grad_norm': 0.5010682344436646, 'learning_rate': 1.8000000000000001e-06, 'num_tokens': 38912.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.18424999713897705, 'rewards/_combined_reward_func/std': 0.013047988526523113, 'reward': -0.18424999713897705, 'reward_std': 0.013047988526523113, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 1.9}\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 19/20 [01:15<00:03,  3.20s/it][REWARD] Format: 0.000, Correctness: 0.000, Length: -0.192, Total: -0.192\n",
      "{'loss': -0.0, 'grad_norm': 0.4366418719291687, 'learning_rate': 1.9000000000000002e-06, 'num_tokens': 40960.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/_combined_reward_func/mean': -0.1837500035762787, 'rewards/_combined_reward_func/std': 0.0234147347509861, 'reward': -0.1837500035762787, 'reward_std': 0.02341473288834095, 'frac_reward_zero_std': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 2.0}\n",
      "{'train_runtime': 79.5073, 'train_samples_per_second': 0.252, 'train_steps_per_second': 0.252, 'train_loss': 3.837049007415772e-08, 'epoch': 2.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:19<00:00,  3.98s/it]\n",
      "GRPO training completed successfully\n",
      "GRPO training completed!\n",
      "Inference model cleaned up\n",
      "[rank0]:[W719 18:24:30.808493063 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Train GRPO: /jupyter-tutorial/hf_models/Llama-3.2-1B-Instruct\n",
    "!python -m trainer \\\n",
    "    --training_type grpo \\\n",
    "    --mode train \\\n",
    "    --output_dir \"checkpoints/grpo\" \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --lora_r 16 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --vllm_gpu_memory_utilization 0.7 \\\n",
    "    --disable_wandb # remove this or comment out for wandb logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f99808",
   "metadata": {},
   "source": [
    "**For more such use cases, go through `__main__` block of [trainer.py](trainer.py)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb90b4",
   "metadata": {},
   "source": [
    "If SFT ckpt is used for GRPO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6748d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to bring your own code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747fe63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945b5b67",
   "metadata": {},
   "source": [
    "### Prompt Tuning\n",
    "\n",
    "In this section we'll see how you efficiently communicate with your model (*make your thoughts visible*) to obtain what you desired. Just like the following image ðŸ˜œ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38669e2a",
   "metadata": {},
   "source": [
    "<img src=\"../assets/prompt.jpg\">\n",
    "\n",
    "Pic Credits: [Edurado Ordax](https://www.linkedin.com/in/eordax/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6ed03",
   "metadata": {},
   "source": [
    "*The following prompt-tuning can be tested out at [question_agent.py](../agents/question_agent.py) `__main__` code block.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demonstration of how tuning-prompt improves the model output.\n",
    "\n",
    "# Basic prompt:\n",
    "sys_prompt1 = \"You are an examiner tasked with creating extremely difficult multiple-choice questions\"\n",
    "\n",
    "output1 = \"\"\"\n",
    "        {\n",
    "            \"question\": \"Identify the next number: 23, 43, 73, 113, ?\",\n",
    "            \"choices\": [\n",
    "                \"A) 163\",\n",
    "                \"B) 173\",\n",
    "                \"C) 157\",\n",
    "                \"D) 167\"\n",
    "            ],\n",
    "            \"answer\": \"A) 163\",\n",
    "            \"explanation\": \"The differences between terms increase by 10 each time: 43-23=20, 73-43=30, 113-73=40, so next difference is 50. 113+50=163, which is also a prime.\"\n",
    "        }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# After we tune the prompt as per our requirement (i.e., conveying model our intentions) we get following:\n",
    "sys_prompt2 = \"\"\"\n",
    "    You are an **expert-level examiner** with deep expertise in designing **highly challenging and conceptually rigorous multiple-choice questions (MCQs)** for the **Quantitative Aptitude and Analytical Reasoning** sections of top-tier competitive exams.\n",
    "    Think step by step to generate the question and solve the same, but only output the final answer. Do not show your thinking process.\n",
    "    **Please DO NOT reveal the solution steps or any intermediate reasoning.**\n",
    "\"\"\"\n",
    "\n",
    "# Output: \n",
    "output2 = \"\"\"\n",
    "        {\n",
    "            \"question\": \"What is the next term in the series: 2, 5, 10, 15, 90, 97, ?, 2339\",\n",
    "            \"choices\": [\n",
    "                \"A) 582\",\n",
    "                \"B) 1164\",\n",
    "                \"C) 1746\",\n",
    "                \"D) 2328\"\n",
    "            ],\n",
    "            \"answer\": \"D) 2328\",\n",
    "            \"explanation\": \"The pattern alternates between adding the next prime number and multiplying by the next factorial: 2 + 3 = 5, 5 x 2! = 10, 10 + 5 = 15, 15 x 3! = 90, 90 + 7 = 97, 97 x 4! = 2328, 2328 + 11 = 2339. So the missing term is 97 x 24 = 2328.\"\n",
    "        }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: Here the `sys_prompt` denotes the system-prompt which sets the context, tone, and boundaries for the AI's actions, shaping its overall conduct throughout the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4643dfa",
   "metadata": {},
   "source": [
    "##### Examples of Prompt-tuning:\n",
    "1. CoT\n",
    "2. Few-Shot (In-context) prompting\n",
    "3. Self-consistency decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8730f",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">Tips and Tricks</span>:\n",
    "\n",
    "*   SFT/GRPO/Prompt-finetuning (also Distillation) for improved response from agents. This\n",
    "    *   Ensure format correctness\n",
    "    *   Ensure question-choices-answer correctness\n",
    "    *   Improve question difficulty\n",
    "    *   Improve answer scoring\n",
    "    *   Try improving reasoning ability\n",
    "    *   Create a good training dataset (with reasoning traces maybe)\n",
    "*   *Datasets if required can be sourced through internet or generated using Frontier models.*\n",
    "*   <span style=\"color: green\">Try</span> dividing the aspects for improvements among yourselves as much as possible - *Team that works together, wins together*ðŸ†.\n",
    "*   Finally, <span style=\"color : teal\">*Like catches win matches - similarly tips wins patches*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f89cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
